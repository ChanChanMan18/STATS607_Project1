#' Perform 2-fold cross-validation IMPROPERLY; in particular, perform
#' preprocessing step BEFORE data splitting according to the main
#' example of Moscovich and Rosset (2022). The covariates with the K
#' largest empirical variance are selected for prediction prior to 
#' data splitting.
#' 
#' Input:
#' data_list - Results of simulation as generated by generate_sample
#' p - Number of predictors before preprocessing performed
#' K - Number of predictors to be selected according to largest variance
#' sample_size - number of simulated data points
#' seed - seed for reproducibility
#' 
#' Output:
#' MSE_Mean_Incorrect - Average of all MSEs across folds for incorrect procedure

source('src/feature_select_var.R')

improper_cv_2fold <- function(data_list, p, K = 10, sample_size, seed = NULL) {
  
  output_values <- data_list$y
  covariates <- data_list$X
  
  if (length(output_values) != nrow(covariates)) stop("Length of y must match nrow(covariates)")
  if (is.null(sample_size)) sample_size <- nrow(covariates)
  if (sample_size != nrow(covariates)) stop("For LOO CV, sample_size must equal nrow(covariates)")
  if (length(p) > 1 || !is.numeric(p) || !(p > 0) || !(p %% 1 == 0)) stop("p must be a positive whole number")
  
  # Selecting K variables with highest empirical variance BEFORE cv
  sel <- feature_select_var(covariates, K)  
  Xk <- sel$X_K_highest_var
  
  # Make two roughly equal folds, set seed for reproducibility
  if(!is.null(seed)) set.seed(seed)
  idx_all <- sample.int(sample_size)
  folds <- split(idx_all, rep(1:2, length.out = sample_size))
  
  mse_fold <- numeric(2)
  
  # Train on one fold's complement, test on that fold
  for (f in 1:2) {
    test_idx  <- folds[[f]]
    train_idx <- setdiff(seq_len(sample_size), test_idx)
    
    training_data <- as.data.frame(Xk[train_idx, , drop = FALSE])
    testing_data  <- as.data.frame(Xk[test_idx,  , drop = FALSE])
    
    # Ensure identical predictor names in train/test
    colnames(training_data) <- colnames(testing_data) <- paste0("X", seq_len(K))
    
    y_train <- output_values[train_idx]
    y_test  <- output_values[test_idx]
    
    all_data_training <- cbind(training_data, output_values_train = y_train)
    
    # Fit (no intercept)
    fit <- lm(output_values_train ~ . + 0, data = all_data_training)
    
    # Predict on the test fold
    y_hat <- predict(fit, newdata = testing_data)
    
    # Fold MSE
    mse_fold[f] <- mean((y_test - y_hat)^2)
  }
  
  # Compute mean across folds
  MSE_Mean_Incorrect <- mean(mse_fold)
  return(MSE_Mean_Incorrect)
}